{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77573734-3f85-419d-b9a4-6b6ebde466de",
   "metadata": {},
   "source": [
    "# Documentation for Python Script: Predicting Energy Demand\n",
    "\n",
    "This Python script demonstrates how to collect historical energy demand data, store it in an Azure SQL Database, preprocess the data, and use a Random Forest Regressor model to predict energy demand for the next 24 hours. It also connects to an external website to scrape data, uses BeautifulSoup for web scraping, and leverages the pyodbc library for database interactions.\n",
    "\n",
    "\n",
    "## Execute the script. It will perform the following steps:\n",
    "\n",
    "1. Connect to the Azure SQL Database.\n",
    "2. Create the required table.\n",
    "3. Scrape historical energy demand data from a website.\n",
    "4. Insert the scraped data into the database.\n",
    "5. Fetch historical demand and weather data from the database.\n",
    "6. Preprocess the data.\n",
    "7. Train a Random Forest Regressor model and make predictions.\n",
    "8. Calculate and display MAE and RMSE.\n",
    "9. Disconnect from the database.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this script, make sure you have the following prerequisites in place:\n",
    "\n",
    "1. **Python**: You need Python installed on your system. You can download and install Python from the [official Python website](https://www.python.org/downloads/).\n",
    "\n",
    "2. **Required Libraries**: Install the necessary Python libraries using `pip`:\n",
    "```\n",
    "pip install pyodbc requests beautifulsoup4 python-dateutil pandas scikit-learn\n",
    "```\n",
    "   \n",
    "### Usage\n",
    "\n",
    "**Constants and Parameters**: At the beginning of the script specify constants and parameters, including database connection details (server, database, username, password), and the ODBC driver used for connecting to the Azure SQL Database.\n",
    "\n",
    "```python\n",
    "server = \"hdcom1ser.database.windows.net\"\n",
    "database = \"hdcom1\"\n",
    "username = \"serveradminhd\"\n",
    "password = \"A@123456a\"\n",
    "driver = \"{ODBC Driver 18 for SQL Server}\"\n",
    "```\n",
    "\n",
    "**DatabaseConnection Class**: The DatabaseConnection class handles database connection, disconnection, and executing SQL queries.\n",
    "\n",
    "        - connect(): Connects to the Azure SQL Database using the provided credentials.\n",
    "        - disconnect(): Disconnects from the database.\n",
    "        - execute(sql, params=None): Executes an SQL query, optionally with parameters.\n",
    "\n",
    "**create_azure_sql_table Function**: This function creates a table in the Azure SQL Database if it doesn't exist. Modify the table schema as needed.\n",
    "\n",
    "**scrape_data_to_list Function**: This function scrapes historical energy demand data from a website and returns it as a list.\n",
    "\n",
    "**insert_data_to_azure_sql Function**: This function inserts scraped data into the Azure SQL Database table.\n",
    "\n",
    "**fetch_data_from_azure_sql Function**: This function retrieves data from the Azure SQL Database based on a specified time range.\n",
    "\n",
    "**preprocess_data Function**: This function preprocesses the fetched data, including converting timestamps, adding day of the week and hour columns, and merging with weather data.\n",
    "\n",
    "**train_and_predict_model Function**: This function trains a Random Forest Regressor model and predicts energy demand for the next 24 hours. It also calculates mean absolute error (MAE) and root mean squared error (RMSE).\n",
    "\n",
    "**Main Execution Flow**: The main execution flow of the script connects to the database, creates the required table, scrapes data, inserts it into the database, fetches data, preprocesses it, trains the model, makes predictions, and disconnects from the database.\n",
    "Disconnect from the Database: Ensure you disconnect from the Azure SQL Database when you are done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b153d-73a9-42ad-93e1-a12d2f78bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Constants and Parameters\n",
    "server = \"hdcom1ser.database.windows.net\"\n",
    "database = \"hdcom1\"\n",
    "username = \"serveradminhd\"\n",
    "password = \"A@123456a\"\n",
    "driver = \"{ODBC Driver 18 for SQL Server}\"\n",
    "base_url = \"http://ets.aeso.ca/ets_web/ip/Market/Reports/ActualForecastWMRQHReportServlet\"\n",
    "conn_str = f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password};ncrypt=yes;TrustServerCertificate=no'\n",
    "\n",
    "\n",
    "class DatabaseConnection:\n",
    "    def __init__(self):\n",
    "        self.conn_str = conn_str\n",
    "        self.conn = None\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.conn = pyodbc.connect(self.conn_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to the database: {e}\")\n",
    "\n",
    "    def disconnect(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "\n",
    "    def execute(self, sql, params=None):\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            if params:\n",
    "                cursor.execute(sql, params)\n",
    "            else:\n",
    "                cursor.execute(sql)\n",
    "            self.conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing SQL query: {e}\")\n",
    "\n",
    "def create_azure_sql_table(connection):\n",
    "    # Drop the table if it exists\n",
    "    drop_table_query = f\"IF OBJECT_ID('historical_demand_data', 'U') IS NOT NULL BEGIN DROP TABLE historical_demand_data END\"\n",
    "    connection.execute(drop_table_query)\n",
    "\n",
    "    create_table_query = f\"CREATE TABLE historical_demand_data (Datetime DATETIME, Forecast_Pool_Price FLOAT, Actual_Posted_Pool_Price FLOAT, Forecast_AIL FLOAT, Actual_AIL FLOAT, Forecast_AIL_and_Actual_AIL_Difference FLOAT)\"\n",
    "    connection.execute(create_table_query)\n",
    "\n",
    "def get_web_page_content(start_timestamp, end_timestamp):\n",
    "    params = {\n",
    "        \"beginDate\": start_timestamp.strftime('%m%d%Y'),\n",
    "        \"endDate\": end_timestamp.strftime('%m%d%Y'),\n",
    "        \"contentType\": \"html\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f'Failed to retrieve data from the website. Status Code: {response.status_code}')\n",
    "            \n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f'An error occurred during the request: {str(e)}')\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred while fetching web page content: {str(e)}')\n",
    "        \n",
    "        return None\n",
    "\n",
    "def parse_web_page_content(content):\n",
    "    if content is not None:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        table = soup.find('table', attrs={'border': '1', 'align': 'CENTER'})\n",
    "        \n",
    "        return table\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_data_from_table(table):\n",
    "    if table is not None:\n",
    "        data = []\n",
    "\n",
    "        for row in table.find_all('tr'):\n",
    "            columns = row.find_all('td')\n",
    "            row_data = []\n",
    "\n",
    "            for column in columns:\n",
    "                field = column.text.strip()\n",
    "\n",
    "                if '/' in field:\n",
    "                    field = datetime.strptime(field.replace(' 24', ' 00'), '%m/%d/%Y %H')\n",
    "                else:\n",
    "                    try:\n",
    "                        field = float(field.replace(',', ''))\n",
    "                    except ValueError:\n",
    "                        field = 0\n",
    "\n",
    "                row_data.append(field)\n",
    "            data.append(row_data)\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scrape_data_to_list(start_timestamp, end_timestamp):\n",
    "    content = get_web_page_content(start_timestamp, end_timestamp)\n",
    "    table = parse_web_page_content(content)\n",
    "    data = extract_data_from_table(table)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def insert_data_to_azure_sql(connection, data, table_name):\n",
    "    if data is not None:\n",
    "        for row_data in data[1:]:\n",
    "            sql = f\"INSERT INTO {table_name} VALUES (?, ?, ?, ?, ?, ?)\"\n",
    "            connection.execute(sql, row_data)\n",
    "\n",
    "        print(f'Data has been saved to {table_name} in Azure SQL Database.')\n",
    "\n",
    "def fetch_data_from_azure_sql(connection, start_timestamp, end_timestamp, table_name):\n",
    "    query = f\"SELECT * FROM {table_name} WHERE Datetime >= ? AND Datetime <= ?\"\n",
    "    data = pd.read_sql_query(query, connection.conn, params=[start_timestamp, end_timestamp])\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data, weather_data):\n",
    "    data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "    data = data.merge(weather_data, on='Datetime')\n",
    "    data['Datetime'] = data['Datetime'].astype(int) // 10**9\n",
    "    non_numeric_columns = data.select_dtypes(exclude=['number'])\n",
    "    data = data.drop(non_numeric_columns, axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def train_and_predict_model(data):\n",
    "    X = data.drop(['Actual_Posted_Pool_Price'], axis=1)\n",
    "    y = data['Actual_Posted_Pool_Price']\n",
    "\n",
    "    X_train = X.iloc[:-23]\n",
    "    y_train = y.iloc[:-23]\n",
    "    X_val = X.iloc[-23:]\n",
    "    y_val = y.iloc[-23:]\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "    print(f'Predicted Demand for the Next 24 Hours: {y_pred}')\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "    print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "\n",
    "# Main Execution Flow\n",
    "start_timestamp = datetime.now() - relativedelta(years=1)\n",
    "end_timestamp = start_timestamp + timedelta(days=2)\n",
    "\n",
    "connection = DatabaseConnection()\n",
    "connection.connect()\n",
    "\n",
    "create_azure_sql_table(connection)\n",
    "\n",
    "scraped_data = scrape_data_to_list(start_timestamp, end_timestamp)\n",
    "if scraped_data:\n",
    "    insert_data_to_azure_sql(connection, scraped_data, 'historical_demand_data')\n",
    "\n",
    "demand_data = fetch_data_from_azure_sql(connection, start_timestamp, end_timestamp, 'historical_demand_data')\n",
    "weather_data = fetch_data_from_azure_sql(connection, start_timestamp, end_timestamp, 'calgary_weather_data')\n",
    "\n",
    "preprocessed_data = preprocess_data(demand_data, weather_data)\n",
    "train_and_predict_model(preprocessed_data)\n",
    "\n",
    "# Disconnect from the database when done\n",
    "connection.disconnect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
